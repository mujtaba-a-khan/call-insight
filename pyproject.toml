# Project configuration for Call Insights
# Single-file to keep tooling, deps, and metadata in one place.
# The app is build with Streamlit UI, offline Whisper STT, rules-based labels, and a local LLM Q&A layer.

# Packaging notes:
# - I am not using a src/ layout; local modules live in top-level folders.
# - This is a project for my university professor, If you later want to publish a wheel, add __init__.py files to packages you care about.

# Practical reminders:
# - ffmpeg must be on PATH (brew install ffmpeg / apt-get install ffmpeg).
# - If you like to swap the vector DB, confine changes to vectorstore/backend.py and configs/*.yml.

[build-system]
requires = ["setuptools>=69", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "call-insights"
version = "1.1.2"
description = "Streamlit app for audio transcription, deterministic call labeling, and private LLM Q&A."
readme = "README.md"
requires-python = ">=3.13"
license = { text = "MIT" }
authors = [{ name = "Mujtaba Khan" }]

# Core runtime dependencies.
dependencies = [
  # UI
  "streamlit>=1.34",

  # Data / config
  "pandas>=2.1",
  "numpy>=1.26",            # Py3.13 wheels available; downstream libs are compatible now
  "PyYAML>=6.0",

  # Light schema / helpers
  "pydantic>=2.6",

  # Charts
  "matplotlib>=3.8",
  "plotly>=5.22",

  # Audio → text (Whisper via CTranslate2)
  "faster-whisper>=1.0.0",
  "pydub>=0.25.1",
  "audioop-lts>=0.2.1",
  "ffmpeg-python>=0.2.0",

  # Local vector store (Chroma persists under ./vectorstore/chroma)
  "chromadb>=0.5.0",

  # Windows tz database (no-op elsewhere)
  "tzdata>=2024.1; platform_system == 'Windows'",
]

[project.optional-dependencies]
# Local LLM runner via Ollama (install with: pip install .[llm])
llm = [
  "ollama>=0.3.0",
]

# Sentence-Transformers embedder (install with: pip install .[sbert])
# Note: You may want to install a specific Torch build manually for your platform/GPU.
sbert = [
  "sentence-transformers>=2.7.0",
]

# Diarization (install with: pip install .[diarization])
diarization = [
  "resemblyzer>=0.1.3",
  "scikit-learn>=1.4",
]

# Developer tooling (install with: pip install .[dev])
dev = [
  "black>=24.3.0",
  "ruff>=0.4.0",
  "mypy>=1.8.0",
  "pytest>=8.1.0",
  "pytest-cov>=5.0.0",
  "types-PyYAML>=6.0.12.12",
]

[project.scripts]
# Convenience CLI: `callinsights-index` → builds/updates the vector index.
callinsights-index = "scripts.build_vector_index:main"

[tool.setuptools]
# Flat layout; packages are top-level folders.
package-dir = { }

[tool.setuptools.packages.find]
where = ["."]
include = ["call_app*", "core*", "llm*", "vectorstore*", "scripts*", "tests*"]
exclude = ["artifacts*", "cache*"]

[tool.black]
line-length = 100
target-version = ["py313"]
skip-string-normalization = true

[tool.ruff]
line-length = 100
target-version = "py313"
extend-select = ["E", "F", "I"]   # pycodestyle, pyflakes, import sort
ignore = ["E203"]                  # stay compatible with Black
exclude = ["cache", "artifacts", "vectorstore/chroma"]

[tool.mypy]
python_version = "3.13"
ignore_missing_imports = true
strict_optional = true
warn_unused_ignores = true
warn_redundant_casts = true
warn_unreachable = true
exclude = "^(cache|artifacts|vectorstore/chroma)/"

[tool.pytest.ini_options]
minversion = "8.0"
addopts = "-ra -q"
testpaths = ["tests"]
filterwarnings = [
  "ignore::DeprecationWarning",
]